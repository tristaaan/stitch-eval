{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning based methods\n",
    "\n",
    "This is a model which takes a collection of images and figures out how to align them using an unsupervised approach. It is based on a paper by de Vos, Beredensen, Viergever, Sokooti, Staring, Isgum: [https://arxiv.org/pdf/1809.06130.pdf]()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[name: \"/device:CPU:0\"\n",
      "device_type: \"CPU\"\n",
      "memory_limit: 268435456\n",
      "locality {\n",
      "}\n",
      "incarnation: 9343086401542362890\n",
      ", name: \"/device:GPU:0\"\n",
      "device_type: \"GPU\"\n",
      "memory_limit: 3233144832\n",
      "locality {\n",
      "  bus_id: 1\n",
      "  links {\n",
      "  }\n",
      "}\n",
      "incarnation: 16366554192919681538\n",
      "physical_device_desc: \"device: 0, name: GeForce GTX 970, pci bus id: 0000:01:00.0, compute capability: 5.2\"\n",
      "]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.client.session.Session at 0x50d1c18>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import glob\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.client import device_lib\n",
    "print(device_lib.list_local_devices())\n",
    "\n",
    "config = tf.ConfigProto()\n",
    "# config.gpu_options.allocator_type = 'BFC'\n",
    "tf.Session(config = config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['../data/learning\\\\Tp26_Y000_X000_040.tif',\n",
       " '../data/learning\\\\Tp26_Y000_X001_040.tif',\n",
       " '../data/learning\\\\Tp26_Y000_X002_040.tif',\n",
       " '../data/learning\\\\Tp26_Y000_X003_040.tif',\n",
       " '../data/learning\\\\Tp26_Y000_X004_040.tif']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def load_img_paths(target):\n",
    "    '''\n",
    "    Retrieve the full path of all images in the dataset\n",
    "    '''\n",
    "    return glob.glob(target + '/*.tif')\n",
    "\n",
    "data_dir = r'../data'\n",
    "original_data_dir = data_dir + ('/learning')\n",
    "all_files = pd.DataFrame(load_img_paths(original_data_dir))\n",
    "all_files = all_files[0].values.tolist()\n",
    "all_files[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_paths = all_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NCC():\n",
    "    \"\"\"\n",
    "    local (over window) normalized cross correlation\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, win=None, eps=1e-5):\n",
    "        self.win = win\n",
    "        self.eps = eps\n",
    "\n",
    "\n",
    "    def ncc(self, I, J):\n",
    "        # get dimension of volume\n",
    "        # assumes I, J are sized [batch_size, *vol_shape, nb_feats]\n",
    "        ndims = len(I.get_shape().as_list()) - 2\n",
    "        assert ndims in [1, 2, 3], \"volumes should be 1 to 3 dimensions. found: %d\" % ndims\n",
    "\n",
    "        # set window size\n",
    "        if self.win is None:\n",
    "            self.win = [9] * ndims\n",
    "\n",
    "        # get convolution function\n",
    "        conv_fn = getattr(tf.nn, 'conv%dd' % ndims)\n",
    "\n",
    "        # compute CC squares\n",
    "        I2 = I*I\n",
    "        J2 = J*J\n",
    "        IJ = I*J\n",
    "\n",
    "        # compute filters\n",
    "        sum_filt = tf.ones([*self.win, 1, 1])\n",
    "        strides = [1] * (ndims + 2)\n",
    "        padding = 'SAME'\n",
    "\n",
    "        # compute local sums via convolution\n",
    "        I_sum = conv_fn(I, sum_filt, strides, padding)\n",
    "        J_sum = conv_fn(J, sum_filt, strides, padding)\n",
    "        I2_sum = conv_fn(I2, sum_filt, strides, padding)\n",
    "        J2_sum = conv_fn(J2, sum_filt, strides, padding)\n",
    "        IJ_sum = conv_fn(IJ, sum_filt, strides, padding)\n",
    "\n",
    "        # compute cross correlation\n",
    "        win_size = np.prod(self.win)\n",
    "        u_I = I_sum/win_size\n",
    "        u_J = J_sum/win_size\n",
    "\n",
    "        cross = IJ_sum - u_J*I_sum - u_I*J_sum + u_I*u_J*win_size\n",
    "        I_var = I2_sum - 2 * u_I * I_sum + u_I*u_I*win_size\n",
    "        J_var = J2_sum - 2 * u_J * J_sum + u_J*u_J*win_size\n",
    "\n",
    "        cc = cross*cross / (I_var*J_var + self.eps)\n",
    "\n",
    "        # return negative cc.\n",
    "        return tf.reduce_mean(cc)\n",
    "\n",
    "    def loss(self, I, J):\n",
    "        return - self.ncc(I, J)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Network\n",
    "\n",
    "> A ConvNet design for affine image registration. The network analyzes pairs of fixed and moving images in separate pipelines. Ending each pipeline with global average pooling enables analysis of input images of different sizes, and allows concatenation with the fully connected layers that have a fixed number of nodes connected to 12 affne transformation parameter outputs.\n",
    "\n",
    "> The two separate pipelines analyze input pairs of fixed and moving images and each consist of five alternating 3x3x3 convolution layers and 2x2x2 downsampling layers. The number of these layers may vary, depending on task complexity and input image size. The weights of the layers are shared between the two pipelines to limit the number of total parameters in the network.\n",
    "\n",
    "> The Conv-Nets were initialized with Glorot's uniform distribution (Glorot and Bengio, 2010) and optimized with Adam.\n",
    "\n",
    "> Subsequently, the network can be connected to a neural network work that will decode the relative orientations of the fixed and moving images and convert those to 12 affine transformation parameters: *three translation*, *three rotation*, *three scaling*, and *three shearing parameters*.\n",
    "\n",
    "2D images -> Two translation, `x,y`, one rotation `theta`, one scaling `s`, two shearing `gx, gy` = 6 parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers.core import Dense\n",
    "from keras.layers.convolutional import Convolution2D\n",
    "from keras.layers import Input, Conv2D, AveragePooling2D, GlobalAveragePooling2D, concatenate, Reshape\n",
    "from keras.models import Sequential, Model\n",
    "\n",
    "from m_stn import BilinearInterpolation\n",
    "\n",
    "def get_initial_weights(output_size):\n",
    "    b = np.zeros((2, 3), dtype='float32')\n",
    "    b[0, 0] = 1\n",
    "    b[1, 1] = 1\n",
    "    W = np.zeros((output_size, 6), dtype='float32')\n",
    "    weights = [W, b.flatten()]\n",
    "    return weights\n",
    "\n",
    "def dlir_layer(m1, m2, filters):\n",
    "    '''\n",
    "    alternating 3x3 convolution layers and 2x2 downsampling layers\n",
    "    '''\n",
    "    conv= Conv2D(filters, (3,3), activation='relu', padding='same')\n",
    "    avg = AveragePooling2D() # default size is 2x2\n",
    "    return avg(conv(m1)), avg(conv(m2))\n",
    "    \n",
    "def affine_pipeline(input_shape):\n",
    "    '''\n",
    "    five alternating 3x3 convolution layers and 2x2 downsampling layers\n",
    "    Ending each pipeline with global average pooling\n",
    "    '''\n",
    "    filters = 32\n",
    "    in1 = Input(shape=input_shape, name='moving_input')\n",
    "    in2 = Input(shape=input_shape, name='reference_input')\n",
    "    m1, m2 = dlir_layer(in1, in2, filters)\n",
    "    m1, m2 = dlir_layer(m1, m2, filters)\n",
    "    m1, m2 = dlir_layer(m1, m2, filters)\n",
    "    m1, m2 = dlir_layer(m1, m2, filters)\n",
    "    \n",
    "    conv = Conv2D(filters, (3,3), activation='relu', padding='same')\n",
    "    glob_avg = GlobalAveragePooling2D()\n",
    "    \n",
    "    return in1, in2, glob_avg(conv(m1)), glob_avg(conv(m2))\n",
    "    \n",
    "def my_DLIR(input_shape):\n",
    "    '''\n",
    "    Implement DLIR architecture\n",
    "    '''\n",
    "    input_1, input_2, moving_pipeline, reference_pipeline = affine_pipeline(input_shape)\n",
    "    \n",
    "    cat = concatenate([moving_pipeline, reference_pipeline])\n",
    "    cat = Dense(4096, activation='relu')(cat)\n",
    "    affine_transform = Dense(6, activation='linear', weights=get_initial_weights(4096))(cat)\n",
    "    \n",
    "    image_output = BilinearInterpolation(input_shape[:-1])([input_2, affine_transform])\n",
    "    return Model(inputs=[input_1, input_2], outputs=[image_output])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "reference_input (InputLayer)    (None, 195, 256, 1)  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "moving_input (InputLayer)       (None, 195, 256, 1)  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1 (Conv2D)               (None, 195, 256, 32) 320         moving_input[0][0]               \n",
      "                                                                 reference_input[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_1 (AveragePoo (None, 97, 128, 32)  0           conv2d_1[0][0]                   \n",
      "                                                                 conv2d_1[1][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_2 (Conv2D)               (None, 97, 128, 32)  9248        average_pooling2d_1[0][0]        \n",
      "                                                                 average_pooling2d_1[1][0]        \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_2 (AveragePoo (None, 48, 64, 32)   0           conv2d_2[0][0]                   \n",
      "                                                                 conv2d_2[1][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_3 (Conv2D)               (None, 48, 64, 32)   9248        average_pooling2d_2[0][0]        \n",
      "                                                                 average_pooling2d_2[1][0]        \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_3 (AveragePoo (None, 24, 32, 32)   0           conv2d_3[0][0]                   \n",
      "                                                                 conv2d_3[1][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_4 (Conv2D)               (None, 24, 32, 32)   9248        average_pooling2d_3[0][0]        \n",
      "                                                                 average_pooling2d_3[1][0]        \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_4 (AveragePoo (None, 12, 16, 32)   0           conv2d_4[0][0]                   \n",
      "                                                                 conv2d_4[1][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_5 (Conv2D)               (None, 12, 16, 32)   9248        average_pooling2d_4[0][0]        \n",
      "                                                                 average_pooling2d_4[1][0]        \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling2d_1 (Glo (None, 32)           0           conv2d_5[0][0]                   \n",
      "                                                                 conv2d_5[1][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 64)           0           global_average_pooling2d_1[0][0] \n",
      "                                                                 global_average_pooling2d_1[1][0] \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 4096)         266240      concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 6)            24582       dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "bilinear_interpolation_1 (Bilin (None, 195, 256, 1)  0           reference_input[0][0]            \n",
      "                                                                 dense_2[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 328,134\n",
      "Trainable params: 328,134\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.optimizers import Adam\n",
    "from skimage.io import imread\n",
    "\n",
    "def aspect_resize(newsize, shape):\n",
    "    '''\n",
    "    Given an integer and a shape, return a tuple with the longest side of the shape = newsize\n",
    "    '''\n",
    "    m = np.argmax(shape)\n",
    "    if m == 0:\n",
    "        return (newsize, int(shape[1] / (shape[0] / newsize)))\n",
    "    return (int(shape[0] / (shape[1] / newsize)), newsize, 1)\n",
    "\n",
    "orig_shape = imread(all_files[0]).shape\n",
    "img_size = aspect_resize(256, orig_shape)\n",
    "\n",
    "my_model = my_DLIR(img_size)\n",
    "my_model.compile(loss=NCC().loss, optimizer=Adam(lr=1e-4))\n",
    "# my_model.compile(loss='mse', optimizer=Adam(lr=1e-4))\n",
    "my_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tmp directory already exists\n"
     ]
    }
   ],
   "source": [
    "from os import path\n",
    "from keras.callbacks import ModelCheckpoint, Callback, EarlyStopping\n",
    "from keras.preprocessing.image import img_to_array, load_img\n",
    "from keras.utils import Sequence\n",
    "from skimage.transform import resize\n",
    "\n",
    "class LossHistory(Callback):\n",
    "    def on_train_begin(self, logs={}):\n",
    "        self.losses = []\n",
    "        self.val_losses = []\n",
    "\n",
    "    def on_batch_end(self, batch, logs={}):\n",
    "        self.losses.append(logs.get('loss'))\n",
    "        self.val_losses.append(logs.get('val_loss'))\n",
    "        \n",
    "def image_pair_coords(fname, ymin=0, ymax=7, xmin=0, xmax=14):\n",
    "    '''\n",
    "    get an image's pair.\n",
    "    '''\n",
    "    im_coords = path.split(fname)[-1] \\\n",
    "                        .split('.')[0]    \\\n",
    "                        .split('_')[1:3]\n",
    "    y_part = int(im_coords[0].split('Y')[1])\n",
    "    x_part = int(im_coords[1].split('X')[1])\n",
    "    if y_part == ymin:\n",
    "        y_part = ymin+1\n",
    "    elif y_part == ymax:\n",
    "        y_part = ymax-1\n",
    "    else:\n",
    "        y_part  = y_part+1 if np.random.random() > 0.5 else y_part-1\n",
    "\n",
    "    if x_part == xmin:\n",
    "        x_part = xmin+1\n",
    "    elif x_part == xmax:\n",
    "        x_part = xmax-1\n",
    "    else:\n",
    "        x_part = x_part+1 if np.random.random() > 0.5 else x_part-1\n",
    "\n",
    "    ystr = str(y_part).rjust(2,'0')\n",
    "    xstr = str(x_part).rjust(2,'0')\n",
    "    fpath = path.split(fname)[:-1][0]\n",
    "    next_fname = path.join(fpath, 'Tp26_Y0%s_X0%s_040.tif' % (ystr, xstr))\n",
    "    return next_fname       \n",
    "        \n",
    "class DataGenerator(Sequence):\n",
    "    '''\n",
    "    Adapted from https://stanford.edu/~shervine/blog/keras-how-to-generate-data-on-the-fly.html\n",
    "    Allows for multiprocessing in the fit generator\n",
    "    '''\n",
    "\n",
    "    def __init__(self, train_set, val_set, batch_size, im_size):\n",
    "        self.train, self.val = train_set, val_set\n",
    "        self.batch_size = batch_size\n",
    "        self.im_size = im_size\n",
    "\n",
    "    def __len__(self):\n",
    "        return int(np.ceil(len(self.train) / float(self.batch_size)))\n",
    "\n",
    "    # Will output sequence of tuples (image, test) given a datapath\n",
    "    def __getitem__(self, idx):\n",
    "        X1 = np.zeros(shape=(batch_size, self.im_size[0], self.im_size[1], 1))\n",
    "        X2 = np.zeros(shape=(batch_size, self.im_size[0], self.im_size[1], 1))\n",
    "        y  = np.zeros(shape=(batch_size, self.im_size[0], self.im_size[1], 1))\n",
    "        batch = self.train[idx * self.batch_size:(idx + 1) * self.batch_size]\n",
    "        for j,fname in enumerate(batch):\n",
    "            # to speed this up preprocess the images so they aren't resized on the fly\n",
    "            X1[j] = img_to_array(load_img(fname, target_size=self.im_size, grayscale=True))\n",
    "            fname_moving = image_pair_coords(fname)\n",
    "            X2[j] = img_to_array(load_img(fname_moving, target_size=self.im_size, grayscale=True))\n",
    "            y[j] = X1[j]\n",
    "        return ([X1, X2], y)\n",
    "\n",
    "    \n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=5, verbose=1, mode='auto')\n",
    "\n",
    "# create weights file if it doesn't exist for ModelCheckpoint\n",
    "from os import mkdir\n",
    "try: \n",
    "    mkdir('tmp')\n",
    "except FileExistsError:\n",
    "    print('tmp directory already exists')\n",
    "\n",
    "# history function\n",
    "history = LossHistory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/128\n",
      " - 8s - loss: -8.5327e-03\n",
      "Epoch 2/128\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\TWright\\Anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\keras\\callbacks.py:435: RuntimeWarning: Can save best model only with val_loss available, skipping.\n",
      "  'skipping.' % (self.monitor), RuntimeWarning)\n",
      "C:\\Users\\TWright\\Anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\keras\\callbacks.py:526: RuntimeWarning: Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
      "  (self.monitor, ','.join(list(logs.keys()))), RuntimeWarning\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - 3s - loss: 0.0000e+00\n",
      "Epoch 3/128\n",
      " - 3s - loss: 0.0000e+00\n",
      "Epoch 4/128\n",
      " - 3s - loss: 0.0000e+00\n",
      "Epoch 5/128\n",
      " - 3s - loss: 0.0000e+00\n",
      "Epoch 6/128\n",
      " - 3s - loss: 0.0000e+00\n",
      "Epoch 7/128\n",
      " - 3s - loss: 0.0000e+00\n",
      "Epoch 8/128\n",
      " - 3s - loss: 0.0000e+00\n",
      "Epoch 9/128\n",
      " - 4s - loss: 0.0000e+00\n",
      "Epoch 10/128\n",
      " - 3s - loss: 0.0000e+00\n",
      "Epoch 11/128\n",
      " - 3s - loss: 0.0000e+00\n",
      "Epoch 12/128\n",
      " - 3s - loss: 0.0000e+00\n",
      "Epoch 13/128\n",
      " - 4s - loss: 0.0000e+00\n",
      "Epoch 14/128\n",
      " - 3s - loss: 0.0000e+00\n",
      "Epoch 15/128\n",
      " - 3s - loss: 0.0000e+00\n",
      "Epoch 16/128\n",
      " - 3s - loss: 0.0000e+00\n",
      "Epoch 17/128\n",
      " - 3s - loss: 0.0000e+00\n",
      "Epoch 18/128\n",
      " - 3s - loss: 0.0000e+00\n",
      "Epoch 19/128\n",
      " - 3s - loss: 0.0000e+00\n",
      "Epoch 20/128\n",
      " - 3s - loss: 0.0000e+00\n",
      "Epoch 21/128\n",
      " - 3s - loss: 0.0000e+00\n",
      "Epoch 22/128\n",
      " - 3s - loss: 0.0000e+00\n",
      "Epoch 23/128\n",
      " - 3s - loss: 0.0000e+00\n",
      "Epoch 24/128\n",
      " - 3s - loss: 0.0000e+00\n",
      "Epoch 25/128\n",
      " - 3s - loss: 0.0000e+00\n",
      "Epoch 26/128\n",
      " - 3s - loss: 0.0000e+00\n",
      "Epoch 27/128\n",
      " - 3s - loss: 0.0000e+00\n",
      "Epoch 28/128\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-11-0ea5379bab91>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     12\u001b[0m     \u001b[0mworkers\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m     \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 14\u001b[1;33m     \u001b[0mcallbacks\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mhistory\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcheckpointer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mearly_stopping\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     15\u001b[0m )\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\keras\\legacy\\interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name +\n\u001b[0;32m     90\u001b[0m                               '` call to the Keras 2 API: ' + signature, stacklevel=2)\n\u001b[1;32m---> 91\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     93\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[1;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[0;32m   2228\u001b[0m                     outs = self.train_on_batch(x, y,\n\u001b[0;32m   2229\u001b[0m                                                \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2230\u001b[1;33m                                                class_weight=class_weight)\n\u001b[0m\u001b[0;32m   2231\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2232\u001b[0m                     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[1;34m(self, x, y, sample_weight, class_weight)\u001b[0m\n\u001b[0;32m   1881\u001b[0m             \u001b[0mins\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1882\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1883\u001b[1;33m         \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1884\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1885\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0moutputs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2480\u001b[0m         \u001b[0msession\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2481\u001b[0m         updated = session.run(fetches=fetches, feed_dict=feed_dict,\n\u001b[1;32m-> 2482\u001b[1;33m                               **self.session_kwargs)\n\u001b[0m\u001b[0;32m   2483\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2484\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    898\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    899\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 900\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    901\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    902\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1133\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1134\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m-> 1135\u001b[1;33m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[0;32m   1136\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1137\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1314\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1315\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[1;32m-> 1316\u001b[1;33m                            run_metadata)\n\u001b[0m\u001b[0;32m   1317\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1318\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1320\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1321\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1322\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1323\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1324\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1305\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1306\u001b[0m       return self._call_tf_sessionrun(\n\u001b[1;32m-> 1307\u001b[1;33m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[0;32m   1308\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1309\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[1;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[0;32m   1407\u001b[0m       return tf_session.TF_SessionRun_wrapper(\n\u001b[0;32m   1408\u001b[0m           \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1409\u001b[1;33m           run_metadata)\n\u001b[0m\u001b[0;32m   1410\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1411\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mraise_exception_on_not_ok_status\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mstatus\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "batch_size = 1\n",
    "steps_per_epoch  = int(len(train_paths) / batch_size)\n",
    "training_generator = DataGenerator(train_paths, [], batch_size, img_size)\n",
    "\n",
    "# descriptive weight file naming\n",
    "checkpointer = ModelCheckpoint(filepath=('tmp/weights-%d-%d.hdf5' % \n",
    "                                         (batch_size, img_size[0])), \n",
    "                               verbose=1, save_best_only=True)\n",
    "\n",
    "hist = my_model.fit_generator(training_generator,\n",
    "    epochs=128,\n",
    "    workers=3,\n",
    "    verbose=2,\n",
    "    callbacks=[history, checkpointer, early_stopping]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "im1 = resize(imread(all_files[1]), img_size)\n",
    "im2 = resize(imread(all_files[2]), img_size)\n",
    "im1 = im1.reshape(-1, *img_size)\n",
    "im2 = im2.reshape(-1, *img_size)\n",
    "im3 = my_model.predict([im1, im2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.imshow(im3[0][:,:,-1], cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
