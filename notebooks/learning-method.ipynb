{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning based methods\n",
    "\n",
    "This is a model which takes a collection of images and figures out how to align them using an unsupervised approach. It is based on a paper by de Vos, Beredensen, Viergever, Sokooti, Staring, Isgum: [https://arxiv.org/pdf/1809.06130.pdf]()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\TWright\\Anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[name: \"/device:CPU:0\"\n",
      "device_type: \"CPU\"\n",
      "memory_limit: 268435456\n",
      "locality {\n",
      "}\n",
      "incarnation: 12524217794941087837\n",
      ", name: \"/device:GPU:0\"\n",
      "device_type: \"GPU\"\n",
      "memory_limit: 2215219200\n",
      "locality {\n",
      "  bus_id: 1\n",
      "  links {\n",
      "  }\n",
      "}\n",
      "incarnation: 7139961258773231326\n",
      "physical_device_desc: \"device: 0, name: GeForce GTX 970, pci bus id: 0000:01:00.0, compute capability: 5.2\"\n",
      "]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.client.session.Session at 0x4f02978>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import glob\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.client import device_lib\n",
    "print(device_lib.list_local_devices())\n",
    "\n",
    "config = tf.ConfigProto()\n",
    "# config.gpu_options.allocator_type = 'BFC'\n",
    "tf.Session(config = config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['../data/learning\\\\Tp26_Y000_X000_040.tif',\n",
       " '../data/learning\\\\Tp26_Y000_X001_040.tif',\n",
       " '../data/learning\\\\Tp26_Y000_X002_040.tif',\n",
       " '../data/learning\\\\Tp26_Y000_X003_040.tif',\n",
       " '../data/learning\\\\Tp26_Y000_X004_040.tif']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def load_img_paths(target):\n",
    "    '''\n",
    "    Retrieve the full path of all images in the dataset\n",
    "    '''\n",
    "    return glob.glob(target + '/*.tif')\n",
    "\n",
    "data_dir = r'../data'\n",
    "original_data_dir = data_dir + ('/learning')\n",
    "all_files = pd.DataFrame(load_img_paths(original_data_dir))\n",
    "all_files = all_files[0].values.tolist()\n",
    "all_files[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_paths = all_files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Network\n",
    "\n",
    "> A ConvNet design for affine image registration. The network analyzes pairs of fixed and moving images in separate pipelines. Ending each pipeline with global average pooling enables analysis of input images of different sizes, and allows concatenation with the fully connected layers that have a fixed number of nodes connected to 12 affne transformation parameter outputs.\n",
    "\n",
    "> The two separate pipelines analyze input pairs of fixed and moving images and each consist of five alternating 3x3x3 convolution layers and 2x2x2 downsampling layers. The number of these layers may vary, depending on task complexity and input image size. The weights of the layers are shared between the two pipelines to limit the number of total parameters in the network.\n",
    "\n",
    "> The Conv-Nets were initialized with Glorot's uniform distribution (Glorot and Bengio, 2010) and optimized with Adam.\n",
    "\n",
    "> Subsequently, the network can be connected to a neural network work that will decode the relative orientations of the fixed and moving images and convert those to 12 affine transformation parameters: *three translation*, *three rotation*, *three scaling*, and *three shearing parameters*.\n",
    "\n",
    "2D images -> Two translation, `x,y`, one rotation `theta`, two scaling `dx, dy`, two shearing `gx, gy` = 7 parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers.core import Dense\n",
    "from keras.layers.convolutional import Convolution2D\n",
    "from keras.layers import Input, Conv2D, AveragePooling2D, GlobalAveragePooling2D, concatenate\n",
    "from keras.models import Sequential, Model\n",
    "\n",
    "def dlir_layer(m1, m2, filters):\n",
    "    '''\n",
    "    alternating 3x3 convolution layers and 2x2 downsampling layers\n",
    "    '''\n",
    "    conv= Conv2D(filters, (3,3), activation='relu', padding='same')\n",
    "    avg = AveragePooling2D() # default size is 2x2\n",
    "    return avg(conv(m1)), avg(conv(m2))\n",
    "    \n",
    "def affine_pipeline(input_shape):\n",
    "    '''\n",
    "    five alternating 3x3 convolution layers and 2x2 downsampling layers\n",
    "    Ending each pipeline with global average pooling\n",
    "    '''\n",
    "    filters = 32\n",
    "    in1 = Input(shape=input_shape, name='moving_input')\n",
    "    in2 = Input(shape=input_shape, name='reference_input')\n",
    "    m1, m2 = dlir_layer(in1, in2, filters)\n",
    "    m1, m2 = dlir_layer(m1, m2, filters)\n",
    "    m1, m2 = dlir_layer(m1, m2, filters)\n",
    "    m1, m2 = dlir_layer(m1, m2, filters)\n",
    "    \n",
    "    conv = Conv2D(filters, (3,3), activation='relu', padding='same')\n",
    "    glob_avg = GlobalAveragePooling2D()\n",
    "    \n",
    "    return in1, in2, glob_avg(conv(m1)), glob_avg(conv(m2))\n",
    "    \n",
    "def my_DLIR(input_shape):\n",
    "    '''\n",
    "    Implement DLIR architecture\n",
    "    '''\n",
    "    input_1, input_2, moving_pipeline, reference_pipeline = affine_pipeline(input_shape)\n",
    "    \n",
    "    cat = concatenate([moving_pipeline, reference_pipeline])\n",
    "    cat = Dense(2048, activation='relu')(cat)\n",
    "    cat = Dense(7,    activation='linear')(cat)\n",
    "    return Model(inputs=[input_1, input_2], outputs=[cat])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "moving_input (InputLayer)       (None, 114, 150, 1)  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "reference_input (InputLayer)    (None, 114, 150, 1)  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_155 (Conv2D)             (None, 114, 150, 32) 320         moving_input[0][0]               \n",
      "                                                                 reference_input[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_125 (AverageP (None, 57, 75, 32)   0           conv2d_155[0][0]                 \n",
      "                                                                 conv2d_155[1][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_156 (Conv2D)             (None, 57, 75, 32)   9248        average_pooling2d_125[0][0]      \n",
      "                                                                 average_pooling2d_125[1][0]      \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_126 (AverageP (None, 28, 37, 32)   0           conv2d_156[0][0]                 \n",
      "                                                                 conv2d_156[1][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_157 (Conv2D)             (None, 28, 37, 32)   9248        average_pooling2d_126[0][0]      \n",
      "                                                                 average_pooling2d_126[1][0]      \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_127 (AverageP (None, 14, 18, 32)   0           conv2d_157[0][0]                 \n",
      "                                                                 conv2d_157[1][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_158 (Conv2D)             (None, 14, 18, 32)   9248        average_pooling2d_127[0][0]      \n",
      "                                                                 average_pooling2d_127[1][0]      \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_128 (AverageP (None, 7, 9, 32)     0           conv2d_158[0][0]                 \n",
      "                                                                 conv2d_158[1][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_159 (Conv2D)             (None, 7, 9, 32)     9248        average_pooling2d_128[0][0]      \n",
      "                                                                 average_pooling2d_128[1][0]      \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling2d_30 (Gl (None, 32)           0           conv2d_159[0][0]                 \n",
      "                                                                 conv2d_159[1][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_24 (Concatenate)    (None, 64)           0           global_average_pooling2d_30[0][0]\n",
      "                                                                 global_average_pooling2d_30[1][0]\n",
      "__________________________________________________________________________________________________\n",
      "dense_44 (Dense)                (None, 2048)         133120      concatenate_24[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dense_45 (Dense)                (None, 7)            14343       dense_44[0][0]                   \n",
      "==================================================================================================\n",
      "Total params: 184,775\n",
      "Trainable params: 184,775\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.optimizers import Adam\n",
    "from skimage.io import imread\n",
    "\n",
    "def aspect_resize(newsize, shape):\n",
    "    '''\n",
    "    Given an integer and a shape, return a tuple with the longest side of the shape = newsize\n",
    "    '''\n",
    "    m = np.argmax(shape)\n",
    "    if m == 0:\n",
    "        return (newsize, int(shape[1] / (shape[0] / newsize)))\n",
    "    return (int(shape[0] / (shape[1] / newsize)), newsize, 1)\n",
    "\n",
    "orig_shape = imread(all_files[0]).shape\n",
    "img_size = aspect_resize(150, orig_shape)\n",
    "batch_size = 16\n",
    "\n",
    "my_model = my_DLIR(img_size)\n",
    "my_model.compile(loss='mse', optimizer=Adam(lr=1e-5))\n",
    "my_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tmp directory already exists\n"
     ]
    }
   ],
   "source": [
    "from os import path\n",
    "from keras.callbacks import ModelCheckpoint, Callback, EarlyStopping\n",
    "from keras.preprocessing.image import img_to_array, load_img\n",
    "from keras.utils import Sequence\n",
    "from skimage.transform import resize\n",
    "\n",
    "class LossHistory(Callback):\n",
    "    def on_train_begin(self, logs={}):\n",
    "        self.losses = []\n",
    "        self.val_losses = []\n",
    "\n",
    "    def on_batch_end(self, batch, logs={}):\n",
    "        self.losses.append(logs.get('loss'))\n",
    "        self.val_losses.append(logs.get('val_loss'))\n",
    "        \n",
    "def image_pair_coords(fname, ymin=0, ymax=7, xmin=0, xmax=14):\n",
    "    '''\n",
    "    get an image's pair.\n",
    "    '''\n",
    "    im_coords = path.split(fname)[-1] \\\n",
    "                        .split('.')[0]    \\\n",
    "                        .split('_')[1:3]\n",
    "    y_part = int(im_coords[0].split('Y')[1])\n",
    "    x_part = int(im_coords[1].split('X')[1])\n",
    "    if y_part == ymin:\n",
    "        y_part = ymin+1\n",
    "    elif y_part == ymax:\n",
    "        y_part = ymax-1\n",
    "    else:\n",
    "        y_part  = y_part+1 if np.random.random() > 0.5 else y_part-1\n",
    "\n",
    "    if x_part == xmin:\n",
    "        x_part = xmin+1\n",
    "    elif x_part == xmax:\n",
    "        x_part = xmax-1\n",
    "    else:\n",
    "        x_part = x_part+1 if np.random.random() > 0.5 else x_part-1\n",
    "\n",
    "    ystr = str(y_part).rjust(2,'0')\n",
    "    xstr = str(x_part).rjust(2,'0')\n",
    "    fpath = path.split(fname)[:-1][0]\n",
    "    next_fname = path.join(fpath, 'Tp26_Y0%s_X0%s_040.tif' % (ystr, xstr))\n",
    "    return next_fname       \n",
    "        \n",
    "class DataGenerator(Sequence):\n",
    "    '''\n",
    "    Adapted from https://stanford.edu/~shervine/blog/keras-how-to-generate-data-on-the-fly.html\n",
    "    Allows for multiprocessing in the fit generator\n",
    "    '''\n",
    "\n",
    "    def __init__(self, train_set, val_set, batch_size, im_size):\n",
    "        self.train, self.val = train_set, val_set\n",
    "        self.batch_size = batch_size\n",
    "        self.im_size = im_size\n",
    "\n",
    "    def __len__(self):\n",
    "        return int(np.ceil(len(self.train) / float(self.batch_size)))\n",
    "\n",
    "    # Will output sequence of tuples (image, test) given a datapath\n",
    "    def __getitem__(self, idx):\n",
    "        X1 = np.zeros(shape=(batch_size, self.im_size[0], self.im_size[1], 1))\n",
    "        X2 = np.zeros(shape=(batch_size, self.im_size[0], self.im_size[1], 1))\n",
    "        y = np.zeros(shape=(batch_size, 7))\n",
    "        batch = self.train[idx * self.batch_size:(idx + 1) * self.batch_size]\n",
    "        for j,fname in enumerate(batch):\n",
    "            # to speed this up preprocess the images so they aren't resized on the fly\n",
    "            X1[j] = img_to_array(load_img(fname, target_size=self.im_size, grayscale=True))\n",
    "            fname_moving = image_pair_coords(fname)\n",
    "            X2[j] = img_to_array(load_img(fname_moving, target_size=self.im_size, grayscale=True))\n",
    "        return ([X1, X2], y)\n",
    "\n",
    "    \n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=5, verbose=1, mode='auto')\n",
    "\n",
    "# create weights file if it doesn't exist for ModelCheckpoint\n",
    "from os import mkdir\n",
    "try: \n",
    "    mkdir('tmp')\n",
    "except FileExistsError:\n",
    "    print('tmp directory already exists')\n",
    "    \n",
    "# descriptive weight file naming\n",
    "checkpointer = ModelCheckpoint(filepath=('tmp/weights-%d-%d.hdf5' % \n",
    "                                         (batch_size, img_size[0])), \n",
    "                               verbose=1, save_best_only=True)\n",
    "\n",
    "# history function\n",
    "history = LossHistory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      " - 13s - loss: 0.1368\n",
      "Epoch 2/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\TWright\\Anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\keras\\callbacks.py:435: RuntimeWarning: Can save best model only with val_loss available, skipping.\n",
      "  'skipping.' % (self.monitor), RuntimeWarning)\n",
      "C:\\Users\\TWright\\Anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\keras\\callbacks.py:526: RuntimeWarning: Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
      "  (self.monitor, ','.join(list(logs.keys()))), RuntimeWarning\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - 0s - loss: 0.0328\n",
      "Epoch 3/10\n",
      " - 0s - loss: 0.0026\n",
      "Epoch 4/10\n",
      " - 0s - loss: 0.0037\n",
      "Epoch 5/10\n",
      " - 0s - loss: 0.0040\n",
      "Epoch 6/10\n",
      " - 0s - loss: 0.0012\n",
      "Epoch 7/10\n",
      " - 0s - loss: 3.8362e-04\n",
      "Epoch 8/10\n",
      " - 0s - loss: 5.7901e-04\n",
      "Epoch 9/10\n",
      " - 0s - loss: 4.3268e-04\n",
      "Epoch 10/10\n",
      " - 0s - loss: 3.5015e-04\n"
     ]
    }
   ],
   "source": [
    "steps_per_epoch  = int(len(train_paths) / batch_size)\n",
    "training_generator = DataGenerator(train_paths, [], batch_size, img_size)\n",
    "\n",
    "hist = my_model.fit_generator(training_generator,\n",
    "    epochs=10,\n",
    "    workers=3,\n",
    "    verbose=2,\n",
    "    callbacks=[history, checkpointer, early_stopping]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
