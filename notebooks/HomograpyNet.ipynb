{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Homography Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\TWright\\Anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[name: \"/device:CPU:0\"\n",
      "device_type: \"CPU\"\n",
      "memory_limit: 268435456\n",
      "locality {\n",
      "}\n",
      "incarnation: 10148439541308051429\n",
      ", name: \"/device:GPU:0\"\n",
      "device_type: \"GPU\"\n",
      "memory_limit: 3407900672\n",
      "locality {\n",
      "  bus_id: 1\n",
      "  links {\n",
      "  }\n",
      "}\n",
      "incarnation: 2099962373123739046\n",
      "physical_device_desc: \"device: 0, name: GeForce GTX 970, pci bus id: 0000:01:00.0, compute capability: 5.2\"\n",
      "]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.client.session.Session at 0x4cd1c18>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import glob\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.client import device_lib\n",
    "print(device_lib.list_local_devices())\n",
    "\n",
    "config = tf.ConfigProto()\n",
    "# config.gpu_options.allocator_type = 'BFC'\n",
    "tf.Session(config = config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Wrangling\n",
    "\n",
    "Read the directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['../data/learning\\\\Tp26_Y000_X000_040.tif',\n",
       " '../data/learning\\\\Tp26_Y000_X001_040.tif',\n",
       " '../data/learning\\\\Tp26_Y000_X002_040.tif',\n",
       " '../data/learning\\\\Tp26_Y000_X003_040.tif',\n",
       " '../data/learning\\\\Tp26_Y000_X004_040.tif']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def load_img_paths(target):\n",
    "    '''\n",
    "    Retrieve the full path of all images in the dataset\n",
    "    '''\n",
    "    return glob.glob(target + '/*.tif')\n",
    "\n",
    "data_dir = r'../data'\n",
    "original_data_dir = data_dir + ('/learning')\n",
    "all_files = pd.DataFrame(load_img_paths(original_data_dir))\n",
    "train_paths = all_files[0].values.tolist()\n",
    "train_paths[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HomographyNet\n",
    "\n",
    "Regression variant\n",
    "\n",
    "> We use 8 convolutional layers with a max pooling layer (2x2, stride 2) after every two convolutions. The 8 convolutional layers have the following number of filters per layer: 64 [x4], 128 [x4]. The convolutional layers are followed by two fully connected layers. The first fully connected layer has 1024 units. Dropout with a probability of 0.5 is applied after the final convolutional layer and the first fully-connected layer.\n",
    "\n",
    "> The regression network directly produces 8 real-valued\n",
    "numbers and uses the Euclidean (L2) loss as the final layer\n",
    "during training.\n",
    "\n",
    "> using stochastic gradient descent (SGD) with momentum of 0.9. We use a base learning rate of 0.005 and decrease the learning rate by a factor of 10 after every 30,000 iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, Flatten, MaxPooling2D, Input, Activation, Dropout\n",
    "from keras.layers.convolutional import Conv2D\n",
    "import keras.backend as K\n",
    "\n",
    "def euclidean_distance(y_true, y_pred):\n",
    "    return K.sqrt(K.maximum(K.sum(K.square(y_pred - y_true), axis=-1, keepdims=True), K.epsilon()))\n",
    "\n",
    "def conv_block(m, filters):\n",
    "    kernel = (3, 3)\n",
    "    m = Conv2D(filters, kernel, padding='same', activation='relu')(m)\n",
    "    m = Conv2D(filters, kernel, padding='same', activation='relu')(m)\n",
    "    return MaxPooling2D(pool_size=(2, 2), strides=(2, 2))(m)\n",
    "\n",
    "def homography_regression_model(input_dims):\n",
    "    input_shape=(*input_dims, 2)\n",
    "    \n",
    "    input_layer = Input(shape=input_shape, name='input_layer')\n",
    "    \n",
    "    x = conv_block(input_layer, 64)\n",
    "    x = conv_block(x,           64)\n",
    "    x = conv_block(x,           128)\n",
    "    x = conv_block(x,           128)\n",
    "    \n",
    "    x = Conv2D(128, (3,3), padding='same', activation='relu')(x)\n",
    "    x = Conv2D(128, (3,3), padding='same', activation='relu')(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "    \n",
    "    x = Flatten()(x)\n",
    "    \n",
    "    x = Dense(1024, name='FC_1024')(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "    \n",
    "    out = Dense(8, name='output')(x)\n",
    "    \n",
    "    return Model(inputs=input_layer, outputs=[out])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_layer (InputLayer)     (None, 128, 128, 2)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_71 (Conv2D)           (None, 128, 128, 64)      1216      \n",
      "_________________________________________________________________\n",
      "conv2d_72 (Conv2D)           (None, 128, 128, 64)      36928     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_29 (MaxPooling (None, 64, 64, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_73 (Conv2D)           (None, 64, 64, 64)        36928     \n",
      "_________________________________________________________________\n",
      "conv2d_74 (Conv2D)           (None, 64, 64, 64)        36928     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_30 (MaxPooling (None, 32, 32, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_75 (Conv2D)           (None, 32, 32, 128)       73856     \n",
      "_________________________________________________________________\n",
      "conv2d_76 (Conv2D)           (None, 32, 32, 128)       147584    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_31 (MaxPooling (None, 16, 16, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_77 (Conv2D)           (None, 16, 16, 128)       147584    \n",
      "_________________________________________________________________\n",
      "conv2d_78 (Conv2D)           (None, 16, 16, 128)       147584    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_32 (MaxPooling (None, 8, 8, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_79 (Conv2D)           (None, 8, 8, 128)         147584    \n",
      "_________________________________________________________________\n",
      "conv2d_80 (Conv2D)           (None, 8, 8, 128)         147584    \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 8, 8, 128)         0         \n",
      "_________________________________________________________________\n",
      "flatten_8 (Flatten)          (None, 8192)              0         \n",
      "_________________________________________________________________\n",
      "FC_1024 (Dense)              (None, 1024)              8389632   \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "output (Dense)               (None, 8)                 8200      \n",
      "=================================================================\n",
      "Total params: 9,321,608\n",
      "Trainable params: 9,321,608\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.optimizers import SGD\n",
    "from skimage.io import imread\n",
    "\n",
    "img_size = (128, 128)\n",
    "\n",
    "opt = SGD(lr=0.005, decay=1e-6, momentum=0.9)\n",
    "\n",
    "my_model = homography_regression_model(img_size)\n",
    "my_model.compile(optimizer=opt, loss=euclidean_distance)\n",
    "my_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Data Generators\n",
    "\n",
    "We generate the \"seemingly infinite training data\" on the fly by using Keras' DataGenerators."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from os import path\n",
    "from keras.callbacks import ModelCheckpoint, Callback, EarlyStopping\n",
    "from keras.preprocessing.image import img_to_array, load_img\n",
    "from keras.utils import Sequence\n",
    "from skimage.transform import resize, rotate\n",
    "\n",
    "\n",
    "class LossHistory(Callback):\n",
    "    def on_train_begin(self, logs={}):\n",
    "        self.losses = []\n",
    "        self.val_losses = []\n",
    "\n",
    "    def on_batch_end(self, batch, logs={}):\n",
    "        self.losses.append(logs.get('loss'))\n",
    "        self.val_losses.append(logs.get('val_loss'))\n",
    "        \n",
    "class DataGenerator(Sequence):\n",
    "    '''\n",
    "    Adapted from https://stanford.edu/~shervine/blog/keras-how-to-generate-data-on-the-fly.html\n",
    "    Allows for multiprocessing in the fit generator\n",
    "    '''\n",
    "\n",
    "    def __init__(self, train_set, batch_size, im_size):\n",
    "        self.train = train_set\n",
    "        self.batch_size = batch_size\n",
    "        self.im_size = im_size\n",
    "\n",
    "    def __len__(self):\n",
    "        return int(np.ceil(len(self.train) / float(self.batch_size)))\n",
    "\n",
    "    def read_image(self, fname):\n",
    "        return load_img(fname, grayscale=True)\n",
    "    \n",
    "    # Will output sequence of tuples (image, test) given a datapath\n",
    "    def __getitem__(self, idx):\n",
    "        X = np.zeros(shape=(batch_size, self.im_size[0], self.im_size[1], 1))\n",
    "        y  = np.zeros(shape=(batch_size, self.im_size[0], self.im_size[1], 1))\n",
    "        batch = self.train[idx * self.batch_size:(idx + 1) * self.batch_size]\n",
    "        for j,fname in enumerate(batch):\n",
    "            img = self.read_image()\n",
    "\n",
    "        return (X, y)\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=5, verbose=1, mode='auto')\n",
    "\n",
    "# create weights file if it doesn't exist for ModelCheckpoint\n",
    "from os import mkdir\n",
    "try: \n",
    "    mkdir('tmp')\n",
    "except FileExistsError:\n",
    "    print('tmp directory already exists')\n",
    "\n",
    "history = LossHistory()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "> The networks are trained for for 90,000 total iterations using a batch size of 64.\n",
    "\n",
    "No words on how many epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 16\n",
    "epochs     = 50\n",
    "\n",
    "training_generator   = AugmentedDataGenerator(train_paths, batch_size, img_size)\n",
    "# training_generator   = DataGenerator(train_paths, batch_size, img_size)\n",
    "\n",
    "# descriptive weight file naming\n",
    "checkpointer = ModelCheckpoint(filepath=('tmp/weights-%d-%d.hdf5' % \n",
    "                                         (batch_size, img_size[0])), \n",
    "                               verbose=1, save_best_only=True)\n",
    "\n",
    "hist = my_model.fit_generator(training_generator, \n",
    "    epochs=epochs,\n",
    "    workers=3,\n",
    "    verbose=2,\n",
    "    callbacks=[history, checkpointer, early_stopping]\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
